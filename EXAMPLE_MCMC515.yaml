timing: True
debug: False
stop_at_error: False

likelihood:
  lsst_y1.lsst_emu_cs_wcdm:
    emu_file: "./projects/lsst_y1/emulator_output_cs_wcdm/models/model_1"
    data_vector_file: "./projects/lsst_y1/data/example3_lsst_y1_theory.modelvector"
    n_dim: 15 

params:
  logA:
    prior:
      min: 2.83321
      max: 3.21888
    ref:
      dist: norm
      loc: 3.0448
      scale: 0.15
    proposal: 0.15
    latex: \log(10^{10} A_\mathrm{s})
    # drop: true
  # As:
  #   value: 'lambda logA: 1e-10*np.exp(logA)'
  #   latex: A_\mathrm{s}
  #   drop: true
  ns:
    prior:
      min: 0.92
      max: 1.00
    ref:
      dist: norm
      loc: 0.96605
      scale: 0.01
    proposal: 0.01
    latex: n_\mathrm{s}
  H0:
    prior:
      min: 61
      max: 73
    ref:
      dist: norm
      loc: 67.32
      scale: 5
    proposal: 3
    latex: H_0
  omegab:
    prior:
      min: 0.04
      max: 0.06
    ref:
      dist: norm
      loc: 0.0495
      scale: 0.004
    proposal: 0.004
    latex: \Omega_\mathrm{b}
  omegam:
    prior:
      min: 0.24
      max: 0.40
    ref:
      dist: norm
      loc: 0.316
      scale: 0.02
    proposal: 0.02
    latex: \Omega_\mathrm{m}
  # mnu:
  #   value: 0.06
  # tau:
  #   value: 0.05
  #   latex: \tau_\mathrm{reio}
  # omegal:
  #   latex: \Omega_\Lambda
  # As_1e9:
  #   derived: 'lambda As: 1e9*As'
  #   latex: 10^9 A_\mathrm{s}
  # omegam:
  #   latex: \Omega_\mathrm{m}
  # omegamh2:
  #   derived: 'lambda omegam, H0: omegam*(H0/100)**2'
  #   latex: \Omega_\mathrm{m} h^2
  # omegab:
  #   derived: 'lambda omegabh2, H0: omegabh2/((H0/100)**2)'
  #   latex: \Omega_\mathrm{b}
  # omegac:
  #   derived: 'lambda omegach2, H0: omegach2/((H0/100)**2)'
  #   latex: \Omega_\mathrm{c}
  # omegan2:
  #   latex: \Omega_\mathrm{\\nu} h^2
  # omegan:
  #   derived: 'lambda omegan2, H0: omegan2/((H0/100)**2)'
  #   latex: \Omega_\mathrm{\\nu}
  # sigma8:
  #   latex: \sigma_8
  # s8h5:
  #   derived: 'lambda sigma8, H0: sigma8*(H0*1e-2)**(-0.5)'
  #   latex: \sigma_8/h^{0.5}
  # s8omegamp5:
  #   derived: 'lambda sigma8, omegam: sigma8*omegam**0.5'
  #   latex: \sigma_8 \Omega_\mathrm{m}^{0.5}
  # s8omegamp25:
  #   derived: 'lambda sigma8, omegam: sigma8*omegam**0.25'
  #   latex: \sigma_8 \Omega_\mathrm{m}^{0.25}
  # s8:
  #   derived: 'lambda sigma8, omegam: sigma8*(omegam/0.3)**0.5'
  #   latex: \sigma_8 \Omega_\mathrm{m/0.3}^{0.5}
  # LSST_BARYON_Q1:
  #   value: 0.0
  #   latex: Q1_\mathrm{LSST}^1
  # LSST_BARYON_Q2:
  #   value: 0.0
  #   latex: Q2_\mathrm{LSST}^2
  w:
    prior:
      min: -3
      max: -0.01
    ref:
      dist: norm
      loc: -0.99
      scale: 0.05
    proposal: 0.05
    latex: w_{0,\mathrm{DE}}
  w_growth:
    prior:
      min: -1.3
      max: -0.7
    ref:
      dist: norm
      loc: -0.99
      scale: 0.05
    proposal: 0.05
    latex: w_\mathrm{growth}
  omegam_growth:
    prior:
      min: 0.24
      max: 0.4
    ref:
      dist: norm
      loc: 0.3
      scale: 0.01
    proposal: 0.01
    latex: \Omega_\mathrm{m}^\mathrm{growth}

theory:
  # camb:
  #   path: ./external_modules/code/CAMB
  #   stop_at_error: False
  #   use_renames: True
  #   extra_args:
  #     halofit_version: takahashi
  #     AccuracyBoost: 1.15
  #     lens_potential_accuracy: 1.0
  #     num_massive_neutrinos: 1
  #     nnu: 3.046
  #     dark_energy_model: ppf
  #     accurate_massive_neutrino_transfers: false
  #     k_per_logint: 20

sampler:
  mcmc:
    # ---------------------------------------------------------------------
    # File (w/ path) or matrix defining a covariance matrix for the proposal:
    # - null (default): will be generated from params info (prior and proposal)
    # - matrix: remember to set `covmat_params` to the parameters in the matrix
    covmat: "./projects/lsst_y1/EXAMPLE_MCMC3.covmat"
    covmat_params:
    # --------------------------------------
    # --------------------------------------
    # Proposal covariance matrix learning
    # --------------------------------------
    # --------------------------------------
    learn_proposal: True
    # Don't learn if convergence better than...
    learn_proposal_Rminus1_min: 0.03
    # (even earlier if a param is not in the given covariance matrix)
    learn_proposal_Rminus1_max_early: 2000.
    # --------------------------------------
    # --------------------------------------
    # Convergence and stopping
    # --------------------------------------
    # --------------------------------------
    # Maximum number of posterior evaluations
    max_samples: .inf
    # Gelman-Rubin R-1 on means
    Rminus1_stop: 0.02
    # Gelman-Rubin R-1 on std deviations
    Rminus1_cl_stop: 0.2
    Rminus1_cl_level: 0.95
    # --------------------------------------
    # --------------------------------------
    # Exploiting speed hierarchy
    # --------------------------------------
    # --------------------------------------
    measure_speeds: False
    drag: False
    oversample_power: 0.6
    oversample_thin: True
    blocking:
      - [1,
          [
            logA, ns, H0, omegab, omegam, w
          ]
        ]
      - [2,
          [
            LSST_DZ_S1, LSST_DZ_S2, LSST_DZ_S3, LSST_DZ_S4, LSST_DZ_S5, LSST_A1_1, LSST_A1_2, 
            omegam_growth, w_growth
          ]
        ]
      - [25,
          [
            LSST_M1, LSST_M2, LSST_M3, LSST_M4, LSST_M5
          ]
        ]
    # --------------------------------------
    # --------------------------------------
    # Avoid chain getting suck forever
    # --------------------------------------
    # --------------------------------------
    max_tries: 10000
    burn_in: 0
    # ---------------------------------------------------------------------
    # ---------------------------------------------------------------------
    # When no MPI used, number of fractions of the chain to compare
    # ---------------------------------------------------------------------
    # ---------------------------------------------------------------------
    Rminus1_single_split: 4

output: ./projects/lsst_y1/chains/EXAMPLE_MCMC515
